# ğŸ‘» Ghost-Sourcer
**AI-Powered Local Resume Screener for Recruiters**

---

## ğŸ‡§ğŸ‡· PortuguÃªs

Eu desenvolvi o **Ghost-Sourcer** para realizar triagens tÃ©cnicas rodando **100% localmente**. Como Recrutador TÃ©cnico, entendo que a privacidade dos dados dos candidatos Ã© um pilar inegociÃ¡vel da nossa profissÃ£o. Esta ferramenta permite um processamento especializado diretamente no meu hardware, utilizando o poder do **Llama 3**.

### ğŸš€ Por que eu criei esta ferramenta?
* **Privacidade Total**: Desenvolvi o sistema para que o currÃ­culo nunca saia da mÃ¡quina local, garantindo conformidade nativa com **LGPD/GDPR**.
* **EquivalÃªncia Arquitetural**: Fui alÃ©m do mapeamento de palavras-chave; o agente analisa se as competÃªncias tÃ©cnicas sÃ£o transferÃ­veis entre ecossistemas, como validar experiÃªncia em AWS para um stack baseado em GCP.
* **Custo Zero**: Realizo todo o processamento via **Ollama** utilizando minha prÃ³pria GPU.

### ğŸ›¡ï¸ Por que IA Local (Edge)?
Em um ecossistema de recrutamento moderno, a soberania de dados Ã© prioritÃ¡ria. O Ghost-Sourcer foi desenhado para atuar como uma camada de **Edge Computing** para triagem inicial.
* **IngestÃ£o Zero-Trust**: O processamento local garante que informaÃ§Ãµes sensÃ­veis (PII) permaneÃ§am em um ambiente isolado durante a fase de triagem bruta.
* **Complemento Ã  Compliance**: Funciona como um "sandbox" privado, permitindo validar perfis sem a necessidade imediata de trÃ¢nsito de dados em redes externas, ideal para lidar com requisitos rigorosos de privacidade.
* **Mapeamento de PrincÃ­pios**: Ao contrÃ¡rio de filtros estÃ¡ticos, o modelo local foca em **princÃ­pios de engenharia**, identificando talentos que possuem a lÃ³gica necessÃ¡ria para o stack, mesmo que os termos exatos variem.

### ğŸ› ï¸ Minha Stack TÃ©cnica
* **Runtime**: Utilizei **Node.js v24.13.0** para implementar suporte a imports dinÃ¢micos e mÃ³dulos ESM modernos.
* **Backend**: Express.js.
* **AI Engine**: Ollama (Llama 3).
* **Document Parsing**: Implementei o `officeparser` para garantir suporte robusto a arquivos PDF e DOCX.

### ğŸ–¥ï¸ Meu Setup de Hardware
Validei este projeto utilizando meu setup pessoal:
* **CPU**: Ryzen 7 8700F.
* **GPU**: **RTX 5060 Ti**, essencial para manter a latÃªncia de processamento do Llama 3 reduzida.
* **OS**: Windows com Git Bash.

### ğŸ“¦ InstalaÃ§Ã£o e DependÃªncias
Eu ignorei a pasta `node_modules` via `.gitignore` seguindo os padrÃµes da indÃºstria. O arquivo `package.json` contÃ©m todas as definiÃ§Ãµes necessÃ¡rias para que as dependÃªncias sejam instaladas de forma compatÃ­vel com o seu ambiente local.

**Para instalar:**
`npm install`

---

## ğŸ‡ºğŸ‡¸ English

I built **Ghost-Sourcer** to run technical screenings **100% locally**. As a Tech Recruiter at, I recognize that candidate data privacy is a non-negotiable pillar of our field. This tool enables specialized, local processing by leveraging **Llama 3** directly on my own hardware.

### ğŸš€ Why I Built This
* **Total Privacy**: I designed this so resumes never leave your machine, ensuring native **LGPD/GDPR** compliance.
* **Architectural Equivalence**: I moved beyond simple keyword matching; the agent analyzes if technical skills are transferable across ecosystems, such as validating AWS experience for a GCP-based stack.
* **Zero Cost**: I run all processing via **Ollama** on my local GPU.

### ğŸ›¡ï¸ Why Local Edge AI?
In a modern recruiting ecosystem, data sovereignty is a priority. Ghost-Sourcer is designed to act as an **Edge Computing** layer for initial screening.
* **Zero-Trust Data Ingest**: Local processing ensures that PII (Personally Identifiable Information) remains in an isolated environment during the raw screening phase.
* **Complementing Enterprise Compliance**: It functions as a private "sandbox," allowing you to validate profiles without the immediate need for data transit over external networks, ideal for strict privacy requirements.
* **Engineering-First Mapping**: Unlike static filters, the local model focuses on **engineering principles**, identifying talent with the right logic for your stack, even if exact keywords vary across ecosystems.

### ğŸ› ï¸ My Tech Stack
* **Runtime**: I chose **Node.js v24.13.0** to take advantage of modern dynamic imports and ESM modules.
* **Backend**: Express.js.
* **AI Engine**: Ollama (Llama 3).
* **Document Parsing**: I integrated `officeparser` for robust PDF and DOCX support.

### ğŸ–¥ï¸ Hardware Setup
I validated this project on my personal professional setup in my own home:
* **CPU**: Ryzen 7 8700F.
* **GPU**: **RTX 5060 Ti**, which is critical for low-latency Llama 3 processing.
* **OS**: Windows with Git Bash.

### ğŸ“¦ Installation
I excluded the `node_modules` folder via `.gitignore` following standard software engineering practices. The `package.json` file contains all the definitions you need to install the dependencies compatible with your own environment.

**To install:**
`npm install`

---

## ğŸš€ Como Rodar / How to Run
1. **Ollama**: Certifique-se de que o Ollama estÃ¡ ativo com o modelo `llama3` instalado. / Ensure **Ollama** is active with the `llama3` model installed.
2. **Backend**: Inicie o servidor / Start the server: `node server.js`.
3. **UI**: Abra o arquivo `index.html` no seu navegador. / Open `index.html` in your browser.
