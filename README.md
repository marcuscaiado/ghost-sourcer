# ğŸ‘» Ghost-Sourcer
**AI-Powered Local Resume Screener for Recruiters**

---

## ğŸ‡§ğŸ‡· PortuguÃªs

Eu desenvolvi o **Ghost-Sourcer** para realizar triagens tÃ©cnicas rodando **100% localmente**. Como Recrutador TÃ©cnico no Google, entendo que a privacidade dos dados dos candidatos Ã© um pilar inegociÃ¡vel da nossa profissÃ£o. Esta ferramenta elimina a necessidade de nuvens de terceiros ou custos de API, utilizando o poder do **Llama 3** diretamente no meu hardware em minha casa.

### ğŸš€ Por que eu criei esta ferramenta?
* **Privacidade Total**: Desenvolvi o sistema para que o currÃ­culo nunca saia da mÃ¡quina local, garantindo conformidade nativa com **LGPD/GDPR**.
* **EquivalÃªncia Arquitetural**: Fui alÃ©m do mapeamento de palavras-chave; o agente analisa se as competÃªncias tÃ©cnicas sÃ£o transferÃ­veis entre ecossistemas, como validar experiÃªncia em AWS para um stack baseado em GCP.
* **Custo Zero**: Realizo todo o processamento via **Ollama** utilizando minha prÃ³pria GPU.

### ğŸ› ï¸ Minha Stack TÃ©cnica
* **Runtime**: Utilizei **Node.js v24.13.0** para implementar suporte a imports dinÃ¢micos e mÃ³dulos ESM modernos.
* **Backend**: Express.js.
* **AI Engine**: Ollama (Llama 3).
* **Document Parsing**: Implementei o `officeparser` para garantir suporte robusto a arquivos PDF e DOCX.

### ğŸ–¥ï¸ Meu Setup de Hardware
Validei este projeto utilizando meu setup profissional pessoal:
* **CPU**: Ryzen 7 8700F.
* **GPU**: **RTX 5060 Ti**, essencial para manter a latÃªncia de processamento do Llama 3 reduzida.
* **OS**: Windows com Git Bash.

### ğŸ“¦ InstalaÃ§Ã£o e DependÃªncias
Eu ignorei a pasta `node_modules` via `.gitignore` seguindo os padrÃµes da indÃºstria. O arquivo `package.json` contÃ©m todas as definiÃ§Ãµes necessÃ¡rias para que as dependÃªncias sejam instaladas de forma compatÃ­vel com o seu ambiente local.

**Para instalar:**
`npm install`

---

## ğŸ‡ºğŸ‡¸ English

I built **Ghost-Sourcer** to run technical screenings **100% locally**. As a Tech Recruiter at Google, I recognize that candidate data privacy is a non-negotiable pillar of our field. This tool removes reliance on third-party clouds and costly APIs by leveraging **Llama 3** directly on local hardware.

### ğŸš€ Why I Built This
* **Total Privacy**: I designed this so resumes never leave your machine, ensuring native **LGPD/GDPR** compliance.
* **Architectural Equivalence**: I moved beyond simple keyword matching; the agent analyzes if technical skills are transferable across ecosystems, such as validating AWS experience for a GCP-based stack.
* **Zero Cost**: I run all processing via **Ollama** on my local GPU.

### ğŸ› ï¸ My Tech Stack
* **Runtime**: I chose **Node.js v24.13.0** to take advantage of modern dynamic imports and ESM modules.
* **Backend**: Express.js.
* **AI Engine**: Ollama (Llama 3).
* **Document Parsing**: I integrated `officeparser` for robust PDF and DOCX support.

### ğŸ–¥ï¸ Hardware Setup
I validated this project on my personal professional setup in my own home:
* **CPU**: Ryzen 7 8700F.
* **GPU**: **RTX 5060 Ti**, which is critical for low-latency Llama 3 processing.
* **OS**: Windows with Git Bash.

### ğŸ“¦ Installation
I excluded the `node_modules` folder via `.gitignore` following standard software engineering practices. The `package.json` file contains all the definitions you need to install the dependencies compatible with your own environment.

**To install:**
`npm install`

---

## ğŸš€ Como Rodar / How to Run
1.  Ensure **Ollama** is active with the `llama3` model installed.
2.  Start my backend server: `node server.js`.
3.  Open `index.html` in your browser to access the UI.
